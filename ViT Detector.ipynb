{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Transformers for Object Detection in PyTorch\n",
        "\n",
        "This Jupyter notebook is based on the blog post [Vision Transformers for Object Detection](https://www.labellerr.com/blog/vision-transformers-for-object-detection/), adapted to use PyTorch instead of TensorFlow/Keras. The original post demonstrates a simple Vision Transformer (ViT) model for bounding box regression on a single object per image, using subsets of the Caltech-101 dataset (cars and faces). We've translated the architecture, data preparation, training, and evaluation to PyTorch.\n",
        "\n",
        "The model treats images as sequences of patches, encodes them with positional embeddings, applies transformer layers, and outputs a 4-dimensional vector for the bounding box (normalized top-left x, top-left y, bottom-right x, bottom-right y).\n",
        "\n",
        "**Note:** This is a simplified object localizer, not a full multi-object detector like DETR. Run this in a Jupyter environment (e.g., Google Colab) with PyTorch installed. We've assumed a GPU is available for faster training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from scipy.io import loadmat\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import StepLR  # Optional for learning rate scheduling\n",
        "import requests  # For downloading the dataset\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import tarfile\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download and Prepare the Dataset\n",
        "\n",
        "The dataset is Caltech-101, focusing on \"cars\" and \"faces\" categories with bounding box annotations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download Caltech-101 dataset if not present\n",
        "if not os.path.exists(\"101_ObjectCategories.tar.gz\"):\n",
        "    url = \"https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp/view?usp=sharing\"\n",
        "    print(\"Note: The direct download link may require manual intervention. Alternatively, download from http://www.vision.caltech.edu/datasets/ and place '101_ObjectCategories.tar.gz' in the current directory.\")\n",
        "    # For automation, you can use gdown if installed: !gdown 137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\n",
        "else:\n",
        "    print(\"Dataset already downloaded.\")\n",
        "\n",
        "# Extract the tar.gz\n",
        "if not os.path.exists(\"101_ObjectCategories\"):\n",
        "    with tarfile.open(\"101_ObjectCategories.tar.gz\", \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "\n",
        "# Prepare directories\n",
        "data_dir = \"data\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "categories = [\"Faces_easy\", \"Motorbikes\"]  # Using Faces_easy and Motorbikes as per blog (cars are Motorbikes in Caltech-101?)\n",
        "\n",
        "# Copy images\n",
        "for category in categories:\n",
        "    category_dir = os.path.join(\"101_ObjectCategories\", category)\n",
        "    dest_dir = os.path.join(data_dir, category)\n",
        "    shutil.copytree(category_dir, dest_dir)\n",
        "\n",
        "# Load annotations (assuming annotation dir exists; Caltech-101 annotations are in separate .mat files)\n",
        "annotation_dir = \"Annotations\"  # Download from http://www.vision.caltech.edu/Image_Datasets/Caltech101/Annotations.tar if needed\n",
        "if not os.path.exists(annotation_dir):\n",
        "    # Placeholder: Download and extract Annotations.tar\n",
        "    pass\n",
        "\n",
        "# Function to load data\n",
        "def load_data(category):\n",
        "    images = []\n",
        "    bboxes = []\n",
        "    img_dir = os.path.join(\"101_ObjectCategories\", category)\n",
        "    ann_dir = os.path.join(annotation_dir, category)\n",
        "    for filename in os.listdir(img_dir):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(img_dir, filename)\n",
        "            ann_path = os.path.join(ann_dir, filename.replace(\".jpg\", \".mat\"))\n",
        "            if os.path.exists(ann_path):\n",
        "                ann = loadmat(ann_path)\n",
        "                bbox = ann[\"box_coord\"][0]  # [x1, y1, x2, y2]\n",
        "                img = cv2.imread(img_path)\n",
        "                img = cv2.resize(img, (224, 224))\n",
        "                # Normalize bbox to [0,1]\n",
        "                h, w = img.shape[:2]\n",
        "                bbox = bbox.astype(np.float32) / np.array([w, h, w, h])\n",
        "                images.append(img)\n",
        "                bboxes.append(bbox)\n",
        "    return np.array(images), np.array(bboxes)\n",
        "\n",
        "# Load categories\n",
        "images_cars, bboxes_cars = load_data(\"Motorbikes\")\n",
        "images_faces, bboxes_faces = load_data(\"Faces_easy\")\n",
        "\n",
        "# Combine\n",
        "x = np.concatenate([images_cars, images_faces], axis=0)\n",
        "y = np.concatenate([bboxes_cars, bboxes_faces], axis=0)\n",
        "\n",
        "# Shuffle and split train/test (90/10)\n",
        "num_samples = len(x)\n",
        "indices = np.random.permutation(num_samples)\n",
        "split = int(0.9 * num_samples)\n",
        "x_train, y_train = x[indices[:split]], y[indices[:split]]\n",
        "x_test, y_test = x[indices[split:]], y[indices[split:]]\n",
        "print(f\"Train samples: {len(x_train)}, Test samples: {len(x_test)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** The blog uses \"car_side\" and \"Faces\", but Caltech-101 has \"Motorbikes\" for cars and \"Faces_easy\". Adjust categories if needed. Annotations must be downloaded separately if not included."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parameters\n",
        "image_size = 224\n",
        "patch_size = 32\n",
        "\n",
        "# Function to extract patches (for visualization)\n",
        "def extract_patches(image, patch_size):\n",
        "    image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)  # [1, C, H, W]\n",
        "    patches = image.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
        "    patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(1, -1, 3 * patch_size * patch_size)\n",
        "    return patches.squeeze(0).numpy()\n",
        "\n",
        "# Display original image and patches\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(x_train[0])\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "patches = extract_patches(x_train[0], patch_size)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[0]}\")\n",
        "print(f\"Elements per patch: {patches.shape[1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[0]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = np.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img)\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Custom Layers and Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for units in hidden_units:\n",
        "            self.layers.append(nn.Linear(units, units))\n",
        "            self.layers.append(nn.GELU())\n",
        "            self.layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "# Note: The blog's MLP is sequential with decreasing units, but for transformer it's [proj*2, proj]\n",
        "\n",
        "class PatchEncoder(nn.Module):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.projection = nn.Linear(3 * patch_size * patch_size, projection_dim)\n",
        "        self.position_embedding = nn.Embedding(num_patches, projection_dim)\n",
        "\n",
        "    def forward(self, patches):\n",
        "        positions = torch.arange(0, num_patches, device=patches.device)\n",
        "        projected = self.projection(patches)\n",
        "        encoded = projected + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, projection_dim, num_heads, transformer_units, dropout_rate):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(projection_dim)\n",
        "        self.attention = nn.MultiheadAttention(projection_dim, num_heads, dropout=dropout_rate)\n",
        "        self.ln2 = nn.LayerNorm(projection_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(projection_dim, transformer_units[0]),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(transformer_units[0], transformer_units[1]),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, num_patches, proj_dim) -> transpose to (num_patches, batch, proj_dim) for attention\n",
        "        x1 = self.ln1(x).transpose(0, 1)\n",
        "        attn_output, _ = self.attention(x1, x1, x1)\n",
        "        attn_output = attn_output.transpose(0, 1)\n",
        "        x2 = x + attn_output\n",
        "        x3 = self.ln2(x2)\n",
        "        x3 = self.mlp(x3)\n",
        "        return x2 + x3\n",
        "\n",
        "class ViTObjectDetector(nn.Module):\n",
        "    def __init__(self, input_shape, patch_size, num_patches, projection_dim, num_heads, transformer_units, transformer_layers, mlp_head_units, dropout_rate=0.3):\n",
        "        super(ViTObjectDetector, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.projection_dim = projection_dim\n",
        "        self.patch_encoder = PatchEncoder(num_patches, projection_dim)\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock(projection_dim, num_heads, transformer_units, dropout_rate=0.1) for _ in range(transformer_layers)])\n",
        "        self.ln = nn.LayerNorm(projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        mlp_layers = []\n",
        "        in_features = num_patches * projection_dim\n",
        "        for units in mlp_head_units:\n",
        "            mlp_layers.extend([nn.Linear(in_features, units), nn.GELU(), nn.Dropout(dropout_rate)])\n",
        "            in_features = units\n",
        "        self.mlp_head = nn.Sequential(*mlp_layers)\n",
        "        self.output = nn.Linear(in_features, 4)\n",
        "\n",
        "    def extract_patches(self, images):\n",
        "        batch_size = images.shape[0]\n",
        "        patches = images.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        patches = patches.permute(0, 2, 3, 1, 4, 5).reshape(batch_size, self.num_patches, -1)\n",
        "        return patches\n",
        "\n",
        "    def forward(self, x):\n",
        "        patches = self.extract_patches(x)\n",
        "        encoded = self.patch_encoder(patches)\n",
        "        for block in self.transformer_blocks:\n",
        "            encoded = block(encoded)\n",
        "        representation = self.ln(encoded)\n",
        "        representation = representation.view(representation.size(0), -1)\n",
        "        representation = self.dropout(representation)\n",
        "        features = self.mlp_head(representation)\n",
        "        bbox = self.output(features)\n",
        "        return bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Dataset and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ObjectDetectionDataset(Dataset):\n",
        "    def __init__(self, images, boxes):\n",
        "        self.images = images\n",
        "        self.boxes = boxes\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),  # Converts to [0,1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.transform(self.images[idx])\n",
        "        box = torch.from_numpy(self.boxes[idx]).float()\n",
        "        return img, box\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ObjectDetectionDataset(x_train, y_train)\n",
        "test_dataset = ObjectDetectionDataset(x_test, y_test)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate Model and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hyperparameters from blog\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [projection_dim * 2, projection_dim]\n",
        "transformer_layers = 4\n",
        "mlp_head_units = [2048, 1024, 512, 64, 32]\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "num_epochs = 100\n",
        "\n",
        "# Model\n",
        "model = ViTObjectDetector(\n",
        "    input_shape=(3, image_size, image_size),  # C, H, W\n",
        "    patch_size=patch_size,\n",
        "    num_patches=num_patches,\n",
        "    projection_dim=projection_dim,\n",
        "    num_heads=num_heads,\n",
        "    transformer_units=transformer_units,\n",
        "    transformer_layers=transformer_layers,\n",
        "    mlp_head_units=mlp_head_units\n",
        ").to(device)\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "history = {'loss': [], 'val_loss': []}\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for imgs, boxes in train_loader:\n",
        "        imgs, boxes = imgs.to(device), boxes.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(imgs)\n",
        "        loss = criterion(preds, boxes)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * imgs.size(0)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    history['loss'].append(train_loss)\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, boxes in test_loader:\n",
        "            imgs, boxes = imgs.to(device), boxes.to(device)\n",
        "            preds = model(imgs)\n",
        "            loss = criterion(preds, boxes)\n",
        "            val_loss += loss.item() * imgs.size(0)\n",
        "    val_loss /= len(test_loader.dataset)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"vit_object_detector.pth\")\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "# Plot history\n",
        "plt.plot(history['loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation: IoU Calculation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def bounding_box_iou(box_pred, box_true):\n",
        "    # Boxes: [x1, y1, x2, y2] normalized\n",
        "    x1 = max(box_pred[0], box_true[0])\n",
        "    y1 = max(box_pred[1], box_true[1])\n",
        "    x2 = min(box_pred[2], box_true[2])\n",
        "    y2 = min(box_pred[3], box_true[3])\n",
        "    \n",
        "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    area_pred = (box_pred[2] - box_pred[0]) * (box_pred[3] - box_pred[1])\n",
        "    area_true = (box_true[2] - box_true[0]) * (box_true[3] - box_true[1])\n",
        "    union = area_pred + area_true - intersection\n",
        "    \n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(\"vit_object_detector.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Test on a sample\n",
        "with torch.no_grad():\n",
        "    sample_img, sample_box = test_dataset[0]\n",
        "    sample_img = sample_img.unsqueeze(0).to(device)\n",
        "    pred_box = model(sample_img).cpu().squeeze(0).numpy()\n",
        "\n",
        "# Calculate IoU\n",
        "iou = bounding_box_iou(pred_box, sample_box.numpy())\n",
        "print(f\"IoU: {iou:.4f}\")\n",
        "\n",
        "# Visualize\n",
        "def draw_bbox(img, bbox, color=(0, 255, 0)):\n",
        "    h, w = img.shape[:2]\n",
        "    x1, y1, x2, y2 = (bbox * np.array([w, h, w, h])).astype(int)\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "img = x_test[0].copy()\n",
        "draw_bbox(img, sample_box.numpy(), (0, 255, 0))  # Green for ground truth\n",
        "draw_bbox(img, pred_box, (255, 0, 0))  # Red for prediction\n",
        "\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.title(f\"Ground Truth (Green) vs Prediction (Red), IoU: {iou:.4f}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook replicates the blog's functionality in PyTorch. For full multi-object detection, consider extending to models like DETR. Adjust hyperparameters or add data augmentation for better performance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}