{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hands-on Practice with Vision Transformers\n",
        "\n",
        "This document provides step-by-step exercises and solutions for working with Vision Transformers (ViT) in computer vision tasks. These exercises are designed to be compatible with both Google Colab (recommended for users without a GPU) and local environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Image Classification with Pre-trained ViT\n",
        "\n",
        "In this exercise, you’ll use a pre-trained Vision Transformer model to classify images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch torchvision matplotlib\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load pre-trained ViT model and feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Load and Process an Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to load an image from URL\n",
        "def load_image_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    return Image.open(BytesIO(response.content))\n",
        "\n",
        "# Load a sample image\n",
        "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = load_image_from_url(image_url)\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare image for the model\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
        "\n",
        "# Get top 5 predictions\n",
        "probabilities = torch.nn.functional.softmax(logits, dim=-1)[0]\n",
        "top5_prob, top5_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "# Display top 5 predictions\n",
        "for i, (prob, idx) in enumerate(zip(top5_prob, top5_indices)):\n",
        "    print(f\"#{i+1}: {model.config.id2label[idx.item()]} ({prob.item()*100:.2f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution Analysis\n",
        "\n",
        "This exercise demonstrates how to use a pre-trained ViT model for image classification. The model was trained on ImageNet and can recognize 1,000 different classes. The feature extractor handles all the necessary preprocessing, including resizing the image to 224x224 pixels and normalizing the pixel values.\n",
        "\n",
        "The model’s architecture divides the image into 16x16 patches, processes them through a transformer encoder, and uses the [CLS] token’s output for classification. This approach allows the model to capture global relationships between different parts of the image.\n",
        "\n",
        "In Vision Transformers, the attention mechanism computes the relationship between patches using the following equation:\n",
        "\n",
        "\\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) \\cdot V\\]\n",
        "\n",
        "Where $Q$, $K$, and $V$ are the query, key, and value matrices derived from the image patches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Fine-tuning ViT on a Custom Dataset\n",
        "\n",
        "In this exercise, you’ll fine-tune a pre-trained ViT model on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets torch torchvision matplotlib\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load CIFAR-10 dataset\n",
        "dataset = load_dataset(\"cifar10\")\n",
        "print(dataset)\n",
        "\n",
        "# Define class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Load feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "# Define image transformations\n",
        "normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define function to preprocess images\n",
        "def preprocess_train(examples):\n",
        "    examples['pixel_values'] = [train_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
        "    examples['labels'] = examples['label']\n",
        "    return examples\n",
        "\n",
        "def preprocess_val(examples):\n",
        "    examples['pixel_values'] = [val_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
        "    examples['labels'] = examples['label']\n",
        "    return examples\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = dataset['train'].with_transform(preprocess_train)\n",
        "test_dataset = dataset['test'].with_transform(preprocess_val)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Load Pre-trained Model for Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load pre-trained model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224',\n",
        "    num_labels=10,\n",
        "    id2label={str(i): class_names[i] for i in range(10)},\n",
        "    label2id={class_names[i]: str(i) for i in range(10)}\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Define Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define training function\n",
        "def train(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Get inputs\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Get inputs\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "            # Update statistics\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Train and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set training parameters\n",
        "num_epochs = 5\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Train\n",
        "    train_loss = train(model, train_dataloader, optimizer, scheduler, device)\n",
        "\n",
        "    # Evaluate\n",
        "    train_accuracy = evaluate(model, train_dataloader, device)\n",
        "    test_accuracy = evaluate(model, test_dataloader, device)\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Save the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save the model\n",
        "model.save_pretrained(\"./vit-cifar10\")\n",
        "feature_extractor.save_pretrained(\"./vit-cifar10\")\n",
        "print(\"Model saved to ./vit-cifar10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to visualize predictions\n",
        "def visualize_predictions(model, dataset, feature_extractor, device, num_images=5):\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(20, 4))\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Get a random image\n",
        "        idx = np.random.randint(0, len(dataset))\n",
        "        image = dataset[idx]['img'].convert(\"RGB\")\n",
        "        label = dataset[idx]['label']\n",
        "\n",
        "        # Prepare image for the model\n",
        "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        # Get predicted class\n",
        "        predicted_class_idx = logits.argmax(-1).item()\n",
        "\n",
        "        # Display image and prediction\n",
        "        axes[i].imshow(image)\n",
        "        axes[i].set_title(f\"True: {class_names[label]}\\nPred: {class_names[predicted_class_idx]}\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize some predictions\n",
        "visualize_predictions(model, dataset['test'], feature_extractor, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution Analysis\n",
        "\n",
        "This exercise demonstrates how to fine-tune a pre-trained ViT model on a custom dataset (CIFAR-10). The key steps include:\n",
        "\n",
        "0. Data Preparation : Transforming images to the format expected by the ViT model (224x224 pixels) and applying data augmentation to improve generalization.\n",
        "1. Model Adaptation : Modifying the classification head of the pre-trained model to output 10 classes instead of the original 1,000 ImageNet classes.\n",
        "2. Fine-tuning Strategy : Using a small learning rate (5e-5) to update the model parameters without drastically changing the pre-trained weights.\n",
        "3. Evaluation : Monitoring both training and test accuracy to ensure the model is learning effectively without overfitting.\n",
        "\n",
        "The fine-tuned model should achieve around 85-90% accuracy on CIFAR-10 after just a few epochs, demonstrating the power of transfer learning with pre-trained Vision Transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Attention Visualization in Vision Transformers\n",
        "\n",
        "In this exercise, you’ll visualize the attention patterns in a Vision Transformer to understand what the model is focusing on when making predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch torchvision matplotlib numpy\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load Pre-trained Model and Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load pre-trained ViT model and feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', output_attentions=True)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load a sample image\n",
        "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "response = requests.get(image_url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Extract Attention Maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare image for the model\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Get model outputs including attention maps\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "# Get prediction\n",
        "logits = outputs.logits\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
        "\n",
        "# Extract attention maps\n",
        "attention_maps = outputs.attentions  # This is a tuple of tensors\n",
        "\n",
        "# Print attention map shapes\n",
        "print(\"Number of layers:\", len(attention_maps))\n",
        "print(\"Attention map shape for first layer:\", attention_maps[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Visualize Attention Maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualize_attention(image, attention_maps, layer_idx=11, head_idx=0):\n",
        "    \"\"\"\n",
        "    Visualize attention for a specific layer and attention head.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        attention_maps: Tuple of attention tensors from model output\n",
        "        layer_idx: Index of the transformer layer to visualize\n",
        "        head_idx: Index of the attention head to visualize\n",
        "    \"\"\"\n",
        "    # Get attention map for specified layer and head\n",
        "    attention = attention_maps[layer_idx][0, head_idx].detach().cpu().numpy()\n",
        "\n",
        "    # We need to exclude the attention to the CLS token\n",
        "    attention = attention[0, 1:]  # Shape: (num_patches)\n",
        "\n",
        "    # Reshape attention to match image patches\n",
        "    num_patches = int(np.sqrt(attention.shape[0]))\n",
        "    attention_map = attention.reshape(num_patches, num_patches)\n",
        "\n",
        "    # Resize image to match attention map visualization\n",
        "    resized_image = image.resize((224, 224))\n",
        "\n",
        "    # Create figure\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
        "\n",
        "    # Plot original image\n",
        "    ax1.imshow(resized_image)\n",
        "    ax1.set_title(\"Original Image\")\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Plot attention map\n",
        "    ax2.imshow(attention_map, cmap='viridis')\n",
        "    ax2.set_title(f\"Attention Map (Layer {layer_idx+1}, Head {head_idx+1})\")\n",
        "    ax2.axis('off')\n",
        "\n",
        "    # Plot overlay\n",
        "    ax3.imshow(resized_image)\n",
        "    ax3.imshow(attention_map, alpha=0.5, cmap='viridis')\n",
        "    ax3.set_title(\"Attention Overlay\")\n",
        "    ax3.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize attention for the last layer, first head\n",
        "visualize_attention(image, attention_maps, layer_idx=11, head_idx=0)\n",
        "\n",
        "# Visualize attention for the last layer, different head\n",
        "visualize_attention(image, attention_maps, layer_idx=11, head_idx=5)\n",
        "\n",
        "# Visualize attention for an earlier layer\n",
        "visualize_attention(image, attention_maps, layer_idx=5, head_idx=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Visualize Attention Across All Heads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def visualize_all_heads(image, attention_maps, layer_idx=11):\n",
        "    \"\"\"\n",
        "    Visualize attention for all heads in a specific layer.\n",
        "\n",
        "    Args:\n",
        "        image: PIL Image\n",
        "        attention_maps: Tuple of attention tensors from model output\n",
        "        layer_idx: Index of the transformer layer to visualize\n",
        "    \"\"\"\n",
        "    # Get attention maps for specified layer\n",
        "    attention = attention_maps[layer_idx][0].detach().cpu().numpy()\n",
        "\n",
        "    # Number of attention heads\n",
        "    num_heads = attention.shape[0]\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(2, 6, figsize=(20, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Plot attention for each head\n",
        "    for head_idx in range(min(num_heads, 12)):\n",
        "        # Get attention map for this head (excluding CLS token)\n",
        "        head_attention = attention[head_idx, 0, 1:]\n",
        "\n",
        "        # Reshape attention to match image patches\n",
        "        num_patches = int(np.sqrt(head_attention.shape[0]))\n",
        "        attention_map = head_attention.reshape(num_patches, num_patches)\n",
        "\n",
        "        # Plot\n",
        "        axes[head_idx].imshow(image.resize((224, 224)))\n",
        "        axes[head_idx].imshow(attention_map, alpha=0.5, cmap='viridis')\n",
        "        axes[head_idx].set_title(f\"Head {head_idx+1}\")\n",
        "        axes[head_idx].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Attention Maps for Layer {layer_idx+1}\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize all heads for the last layer\n",
        "visualize_all_heads(image, attention_maps, layer_idx=11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution Analysis\n",
        "\n",
        "This exercise demonstrates how to visualize attention patterns in Vision Transformers, providing insights into what the model focuses on when making predictions. Key observations include:\n",
        "\n",
        "0. Different Attention Patterns : Each attention head learns to focus on different aspects of the image. Some heads might attend to object shapes, while others focus on textures or colors.\n",
        "1. Layer Progression : Earlier layers tend to capture more local features, while deeper layers develop more global attention patterns that correspond to semantic concepts.\n",
        "2. Interpretability : Attention visualizations can help interpret the model’s decision-making process, showing which parts of the image influenced the classification the most.\n",
        "\n",
        "These visualizations reveal that Vision Transformers, unlike CNNs, can directly model long-range dependencies in images through their self-attention mechanism, allowing them to capture global context more effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Transfer Learning with ViT for Custom Image Classification\n",
        "\n",
        "In this exercise, you’ll apply a pre-trained ViT model to a custom image classification task using transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch torchvision matplotlib datasets\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load and Prepare a Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the Flowers dataset\n",
        "dataset = load_dataset(\"huggan/flowers-102-categories\")\n",
        "print(dataset)\n",
        "\n",
        "# Get class names\n",
        "class_names = dataset['train'].features['label'].names\n",
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Load feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "# Define image transformations\n",
        "normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define function to preprocess images\n",
        "def preprocess_train(examples):\n",
        "    examples['pixel_values'] = [train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "def preprocess_val(examples):\n",
        "    examples['pixel_values'] = [val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = dataset['train'].with_transform(preprocess_train)\n",
        "val_dataset = dataset['validation'].with_transform(preprocess_val)\n",
        "test_dataset = dataset['test'].with_transform(preprocess_val)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16  # Smaller batch size for larger images\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Load Pre-trained Model and Modify for Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load pre-trained model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224',\n",
        "    num_labels=num_classes,\n",
        "    id2label={str(i): class_names[i] for i in range(num_classes)},\n",
        "    label2id={class_names[i]: str(i) for i in range(num_classes)}\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Freeze the feature extractor parameters\n",
        "for param in model.vit.embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for i in range(8):  # Freeze first 8 layers\n",
        "    for param in model.vit.encoder.layer[i].parameters():\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Define Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define training function\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Get inputs\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update statistics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Print progress\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "# Define evaluation function\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Get inputs\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "            # Update statistics\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Train and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Set training parameters\n",
        "num_epochs = 5\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Lists to store metrics\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_accuracy = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate\n",
        "    val_accuracy = evaluate(model, val_dataloader, device)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_accuracy = evaluate(model, test_dataloader, device)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Plot Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot training progress\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot training loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Plot accuracies\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train')\n",
        "plt.plot(val_accuracies, label='Validation')\n",
        "plt.axhline(y=test_accuracy, color='r', linestyle='-', label='Test')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Save the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save the model\n",
        "model.save_pretrained(\"./vit-flowers\")\n",
        "feature_extractor.save_pretrained(\"./vit-flowers\")\n",
        "print(\"Model saved to ./vit-flowers\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution Analysis\n",
        "\n",
        "This exercise demonstrates transfer learning with Vision Transformers on a custom dataset (Flowers-102). Key aspects include:\n",
        "\n",
        "0. Parameter Freezing : By freezing the embedding layer and early transformer blocks, we leverage the pre-trained feature representations while allowing the model to adapt to the new classification task.\n",
        "1. Learning Rate Selection : Using a small learning rate (2e-5) for fine-tuning prevents catastrophic forgetting of the pre-trained knowledge.\n",
        "2. Data Augmentation : Applying random crops and flips to training images helps improve generalization, especially important when working with limited data.\n",
        "3. Performance Monitoring : Tracking both training and validation accuracy helps detect overfitting and determine the optimal number of training epochs.\n",
        "\n",
        "Transfer learning with ViT is particularly effective for specialized image classification tasks, as the pre-trained model has already learned general visual features that can be adapted to new domains with relatively little training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Efficient Inference with Vision Transformers\n",
        "\n",
        "In this exercise, you’ll learn how to optimize a Vision Transformer model for efficient inference, which is particularly important for deployment on resource-constrained environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch torchvision matplotlib optimum onnx onnxruntime\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Load Pre-trained Model and Test Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load pre-trained ViT model and feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load a sample image\n",
        "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "response = requests.get(image_url)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Benchmark Standard Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to measure inference time\n",
        "def benchmark_inference(model, feature_extractor, image, device, num_runs=10):\n",
        "    # Prepare image for the model\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Warm-up run\n",
        "    with torch.no_grad():\n",
        "        _ = model(**inputs)\n",
        "\n",
        "    # Benchmark runs\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate average time\n",
        "    avg_time = (end_time - start_time) / num_runs\n",
        "\n",
        "    # Get prediction\n",
        "    logits = outputs.logits\n",
        "    predicted_class_idx = logits.argmax(-1).item()\n",
        "\n",
        "    return avg_time, predicted_class_idx\n",
        "\n",
        "# Benchmark standard model\n",
        "std_time, std_pred = benchmark_inference(model, feature_extractor, image, device)\n",
        "print(f\"Standard model inference time: {std_time*1000:.2f} ms\")\n",
        "print(f\"Predicted class: {model.config.id2label[std_pred]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Optimize with Torch JIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a JIT traced model\n",
        "def trace_model(model, feature_extractor, device):\n",
        "    # Prepare dummy input\n",
        "    dummy_input = feature_extractor(images=Image.new('RGB', (224, 224)), return_tensors=\"pt\")\n",
        "    dummy_input = {k: v.to(device) for k, v in dummy_input.items()}\n",
        "\n",
        "    # Trace the model\n",
        "    with torch.no_grad():\n",
        "        traced_model = torch.jit.trace(\n",
        "            model, example_kwarg_inputs=dummy_input\n",
        "        )\n",
        "\n",
        "    return traced_model\n",
        "\n",
        "# Trace the model\n",
        "traced_model = trace_model(model, feature_extractor, device)\n",
        "\n",
        "# Benchmark JIT traced model\n",
        "jit_time, jit_pred = benchmark_inference(traced_model, feature_extractor, image, device)\n",
        "print(f\"JIT traced model inference time: {jit_time*1000:.2f} ms\")\n",
        "print(f\"Predicted class: {model.config.id2label[jit_pred]}\")\n",
        "print(f\"Speed improvement: {std_time/jit_time:.2f}x\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Quantize the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quantize the model to int8\n",
        "def quantize_model(model):\n",
        "    quantized_model = torch.quantization.quantize_dynamic(\n",
        "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "    return quantized_model\n",
        "\n",
        "# Try to quantize the model (note: may not work with all models)\n",
        "try:\n",
        "    # Move model to CPU for quantization\n",
        "    cpu_model = model.cpu()\n",
        "\n",
        "    # Quantize\n",
        "    quantized_model = quantize_model(cpu_model)\n",
        "\n",
        "    # Move back to original device\n",
        "    quantized_model = quantized_model.to(device)\n",
        "\n",
        "    # Benchmark quantized model\n",
        "    quant_time, quant_pred = benchmark_inference(quantized_model, feature_extractor, image, device)\n",
        "    print(f\"Quantized model inference time: {quant_time*1000:.2f} ms\")\n",
        "    print(f\"Predicted class: {model.config.id2label[quant_pred]}\")\n",
        "    print(f\"Speed improvement: {std_time/quant_time:.2f}x\")\n",
        "except Exception as e:\n",
        "    print(f\"Quantization failed: {e}\")\n",
        "    print(\"Dynamic quantization may not be supported for this model architecture.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Export to ONNX Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Export model to ONNX format\n",
        "def export_to_onnx(model, feature_extractor):\n",
        "    # Prepare dummy input\n",
        "    dummy_input = feature_extractor(images=Image.new('RGB', (224, 224)), return_tensors=\"pt\")\n",
        "\n",
        "    # Export to ONNX\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        (dummy_input['pixel_values'],),\n",
        "        \"vit_model.onnx\",\n",
        "        input_names=['pixel_values'],\n",
        "        output_names=['logits'],\n",
        "        dynamic_axes={\n",
        "            'pixel_values': {0: 'batch_size'},\n",
        "            'logits': {0: 'batch_size'}\n",
        "        },\n",
        "        opset_version=12\n",
        "    )\n",
        "\n",
        "    return \"vit_model.onnx\"\n",
        "\n",
        "# Try to export the model to ONNX\n",
        "try:\n",
        "    # Move model to CPU for ONNX export\n",
        "    cpu_model = model.cpu()\n",
        "\n",
        "    # Export to ONNX\n",
        "    onnx_path = export_to_onnx(cpu_model, feature_extractor)\n",
        "    print(f\"Model exported to {onnx_path}\")\n",
        "\n",
        "    # Move model back to original device\n",
        "    model = model.to(device)\n",
        "except Exception as e:\n",
        "    print(f\"ONNX export failed: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Inference with Batch Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to benchmark batch inference\n",
        "def benchmark_batch_inference(model, feature_extractor, image, device, batch_size=4, num_runs=10):\n",
        "    # Create a batch of images\n",
        "    images = [image] * batch_size\n",
        "\n",
        "    # Prepare batch for the model\n",
        "    inputs = feature_extractor(images=images, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Warm-up run\n",
        "    with torch.no_grad():\n",
        "        _ = model(**inputs)\n",
        "\n",
        "    # Benchmark runs\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_runs):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate average time per image\n",
        "    avg_time_per_image = (end_time - start_time) / (num_runs * batch_size)\n",
        "\n",
        "    return avg_time_per_image\n",
        "\n",
        "# Benchmark batch inference\n",
        "batch_sizes = [1, 2, 4, 8, 16]\n",
        "batch_times = []\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    try:\n",
        "        avg_time = benchmark_batch_inference(model, feature_extractor, image, device, batch_size=bs)\n",
        "        batch_times.append(avg_time)\n",
        "        print(f\"Batch size {bs}: {avg_time*1000:.2f} ms per image\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Batch size {bs} failed: {e}\")\n",
        "        break\n",
        "\n",
        "# Plot batch inference results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(batch_sizes[:len(batch_times)], [t*1000 for t in batch_times], marker='o')\n",
        "plt.title('Inference Time per Image vs. Batch Size')\n",
        "plt.xlabel('Batch Size')\n",
        "plt.ylabel('Time per Image (ms)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution Analysis\n",
        "\n",
        "This exercise demonstrates various techniques to optimize Vision Transformer models for efficient inference:\n",
        "\n",
        "0. JIT Tracing : Converting the model to TorchScript via tracing can improve inference speed by optimizing the execution graph.\n",
        "1. Quantization : Reducing the precision of model weights from 32-bit floating point to 8-bit integers can significantly decrease memory usage and improve inference speed, with minimal impact on accuracy.\n",
        "2. ONNX Export : Exporting to ONNX format allows the model to be deployed on various hardware and software platforms that support the ONNX runtime.\n",
        "3. Batch Processing : Processing multiple images in a batch can improve throughput by better utilizing hardware parallelism, though there’s a trade-off with memory usage.\n",
        "\n",
        "These optimization techniques are particularly important when deploying Vision Transformers in production environments or on edge devices with limited computational resources. The specific gains will vary depending on the hardware, model size, and implementation details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "These hands-on exercises provide a comprehensive introduction to working with Vision Transformers for computer vision tasks. From basic inference with pre-trained models to fine-tuning on custom datasets, attention visualization, and optimization for efficient deployment, you’ve explored the key aspects of using ViTs in practical applications.\n",
        "\n",
        "Vision Transformers represent a significant advancement in computer vision, offering a different approach from traditional CNNs by leveraging self-attention mechanisms to capture global relationships in images. As demonstrated in these exercises, they can achieve excellent performance across various tasks while providing unique insights through attention visualization.\n",
        "\n",
        "As you continue working with Vision Transformers, remember that they typically perform best when pre-trained on large datasets and then fine-tuned for specific tasks. The transfer learning approach is particularly effective for adapting these powerful models to specialized domains with limited training data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}